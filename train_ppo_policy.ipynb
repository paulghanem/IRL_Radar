{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9df5a9-2577-401c-984d-fd80cf79eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class CustomTerminationWrapper(Wrapper):\n",
    "    def __init__(self, env, max_steps):\n",
    "        super().__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_step = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Override termination condition\n",
    "        # Terminate only if max steps reached\n",
    "        if self.current_step >= self.max_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "      \n",
    "# Create the environment with the wrapper\n",
    "max_steps = 500  # Set this to your desired maximum number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f6d40-a4ab-45be-8c65-76e52bd43549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -7.45    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00564  |\n",
      "|    critic_loss     | 1.14e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -7.35    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 2800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00187  |\n",
      "|    critic_loss     | 7.43e-08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -8.15    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 4200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.00193  |\n",
      "|    critic_loss     | 1.93e-06 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4099     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 350       |\n",
      "|    ep_rew_mean     | -8.22     |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 175       |\n",
      "|    time_elapsed    | 31        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.000662 |\n",
      "|    critic_loss     | 9.42e-08  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -8.29    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00108 |\n",
      "|    critic_loss     | 2.55e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -8.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 8400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00265 |\n",
      "|    critic_loss     | 6.93e-08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -8.28    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 9800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00335 |\n",
      "|    critic_loss     | 2.03e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -8.56    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00341 |\n",
      "|    critic_loss     | 5.4e-08  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11099    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -8.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 12600    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.003   |\n",
      "|    critic_loss     | 2.01e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12499    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -7.92    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 14000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00261 |\n",
      "|    critic_loss     | 9.06e-08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -7.84    |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 15400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00199 |\n",
      "|    critic_loss     | 7.91e-08 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15299    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 350      |\n",
      "|    ep_rew_mean     | -7.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 16800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.00137 |\n",
      "|    critic_loss     | 1.15e-07 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16699    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "max_steps = 350  # Set this to your desired maximum number of steps\n",
    "# env = gym.make('MountainCarContinuous-v0',render_mode='rgb_array') \n",
    "env = CustomTerminationWrapper(gym.make('MountainCarContinuous-v0',render_mode='rgb_array'), max_steps)\n",
    "# env = gym.make(\"whatever\")\n",
    "# Initialize the PPO agent\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1,device='cpu')\n",
    "# The noise objects for DDPG\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.5 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, buffer_size=5000, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=300000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ddpg_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafd22f-89cd-4ac9-ba29-547037c5415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rirl",
   "language": "python",
   "name": "rirl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
